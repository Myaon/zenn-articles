---
title: "WebXR Depth Sensing Moduleについての調査"
emoji: "🕵️‍♂️"
type: "tech" # tech: 技術記事 / idea: アイデア
topics: ["webar", "ar", "javascript", "typescript"]
published: false
---

:::message
本記事は[WebXR アドベントカレンダー](https://adventar.org/calendars/7401)のｎ日目の記事です。
:::

# はじめに

## TL;DR

## 記事の概要と対象読者

本記事では WebXR Depth Sensing Module というものについて解説します。
詳細は後述しますが、ブラウザに実装されている WebAR の API なので WebAR 開発者向けの内容になります。

また本記事では JavaScript・TypeScript・Babylon.js などの技術スタックを説明なしに取り上げますので、それらの知識があると望ましいです。

## 検証環境

筆者が検証に用いた環境を次に示します。

| 環境          | バージョンなど                |
| :------------ | :---------------------------- |
| 開発機        | Windows 10 Home               |
| デバッグ機    | Android 12 Google Pixel 4a 5G |
| Google Chrome | Chrome for Android 106        |
| Node.js       | 16.13.0                       |
| yarn          | 1.22.18                       |
| Vite          | 3.1.0                         |
| TypeScript    | 4                             |
| Babylon.js    | 5.27.0                        |

## サンプルプロジェクト

WebXR Depth Sensing Module を Babylon.js で使ったサンプルプロジェクトを GitHub に公開しておりますので、合わせてご覧ください。

https://github.com/drumath2237/webxr-depth-testbed-babylon

# WebXR Depth Sensing Module の概要

## この Module はいったい何なのか

WebXR Depth Sensing Module とは、WebXR Device API で提供されている Module の一種です。
W3C の Working Draft に概要や使い方などが記されています。

https://www.w3.org/TR/2022/WD-webxr-depth-sensing-1-20220419/

草案の GitHub リポジトリは以下です。

https://github.com/immersive-web/depth-sensing/

## 何ができるのか

この Module は名前の通り、Depth、つまり深度情報を取得するための API を提供します。
深度情報とは WebAR デバイスのカメラ画角において、「ここらへんに見えている物はどのくらいユーザから離れているのか」を表すものです。

例えば iPhone には LiDAR センサーが載っている機種がありますが、LiDAR センサーを使うことで光が飛んでいる時間から光が到達した物体までの距離を割り出すことができます。また LiDAR センサーが載っていない ARCore に対応した Android デバイスも、画像処理によって深度推定をして距離を求められます。
今紹介したものはいずれもネイティブアプリの API によって実現可能ですが、これらをブラウザからでも実現したものが WebXR Depth Sensing Module です。
Depth 画像を使うと物体の形状を 3 次元的に復元したり、AR シーンでオクルージョンを実現したり、空間認識の精度を向上させたりできます。

![img](https://docs-assets.developer.apple.com/published/e2a121be00/rendered2x-1611871073.png)
_Depth 画像の参考（引用：[Apple Developer](https://developer.apple.com/documentation/arkit/environmental_analysis/displaying_a_point_cloud_using_scene_depth)）_

## どうやって使うのか

使い方を知るには GitHub の explaner を見るのが一番簡単です。
基本的な使い方と型定義などが書いてあります。もっと詳しく知りたい場合は W3C の Working Draft を見てみましょう。

https://github.com/immersive-web/depth-sensing/blob/main/explainer.md

### 機能の有効化（リクエスト）

explaner に書いてある通り、WebXRSession を受け取る際にオプションとして DepthSesning の機能をリクエストします。

```js
const session = await navigator.xr.requestSession("immersive-ar", {
  requiredFeatures: ["depth-sensing"],
  depthSensing: {
    usagePreference: ["cpu-optimized", "gpu-optimized"],
    formatPreference: ["luminance-alpha", "float32"],
  },
});
```

ここで特徴的なのは「Usage」と「Format」を指定できる点でしょう。

### Usage と Format

まず Usage ですが、深度データを扱う用途を指定できます。具体的には CPU で処理をする想定なのか、GPU を想定しているかです。扱い方の詳細は後述しますが、Usage によって深度情報をどこから取得して何が返ってくるのかが変わってきますので、用途に合わせて適切なものを選択しましょう。次の Web IDL をご覧ください（explaner から引用）。

```webidl: 深度情報の定義
[SecureContext, Exposed=Window]
interface XRDepthInformation {
  // All methods / attributes are accessible only when the frame
  // that the depth information originated from is active and animated.

  readonly attribute unsigned long width;
  readonly attribute unsigned long height;

  [SameObject] readonly attribute XRRigidTransform normTextureFromNormView;
  readonly attribute float rawValueToMeters;
};

interface XRCPUDepthInformation : XRDepthInformation {
  // Data format is determined by session's depthDataFormat attribute.
  [SameObject] readonly attribute ArrayBuffer data;

  // おそらくここの引数はlongじゃなくてfloatだった気がする（explanerのミス？）
  float getDepthInMeters(unsigned long column, unsigned long row);
};

interface XRWebGLDepthInformation : XRDepthInformation {
  // Opaque texture, its format is determined by session's depthDataFormat attribute.
  [SameObject] readonly attribute WebGLTexture texture;
};
```

ここには`XRDepthInformation`という共通のデータを定義したインタフェースと、それを継承した`XRCPUDepthInformation`と`XRWebGLDepthInformation`がありますね。
名前を良く見ると CPU・WebGL といった単語が入っているのがわかる通り、これらはそれぞれ`cpu-optimize`モードと`gpu-optimize`モードで受け取れるデータ形式です。
CPU モードでは ArrayBuffer による配列で深度配列を取得できますが、GPU モードでは WebGLTexture として深度画像を取得できます。

次に Format ですが、こちらは`luminance-alpha`もしくは`float32`を指定できます。
`luminance-alpha`が指定された場合、深度バッファ（深度配列や深度画像のデータ）は uint16 型（short 型）、つまり 16bit 整数値として返ってきます。`float32`を指定した場合、深度バッファは float 型で 4 バイトの浮動小数で返ってきます。詳しい仕様は Working Draft の表がわかりやすかったので引用します。

![img](/images/depth-sensing-module/depthformat-table.png)
_Format の違いを説明した表（引用：[Working Draft](https://www.w3.org/TR/2022/WD-webxr-depth-sensing-1-20220419/#usage-and-formats)）_

### 深度情報を取得する

<!-- CPU・GPUモードそれぞれで取得方法の違いを説明する -->

深度情報の取得方法は、CPU モードか GPU モードかで変わってきます。
まず CPU モードの場合、深度情報は XRFrame から取得します。

```js: DepthInfoの取得（explanerから引用）
const view = ...;  // Obtained from a viewer pose.
const depthInfo = frame.getDepthInformation(view);

if(depthInfo == null) {
  ... // Handle the case where current frame does not carry depth information.
}

// Obtain the depth at (x, y) normalized view coordinates:
const depthInMeters = depthInfo.getDepthInMeters(x, y);
```

上の例では`getDepthInMeters`メソッドによって画面の UV から距離を 1 つ取得していますね。
CPU モードでは次のように深度配列も受け取れます。`depthInfo.data`は`ArrayBuffer`として渡されるので、適当な TypedArray に解釈させます。

```js: 深度配列の取得（luminance-alphaの場合）
const uint16Data = new Uint16Array(depthInfo.data);

const index = c + r * depthInfo.width;
const depthInMetres = uint16Data[index] * depthInfo.rawValueToMeters;
```

```js: 深度配列の取得（float32の場合）
const float32Data = new Float32Array(depthInfo.data);

const index = c + r * depthInfo.width;
const depthInMetres = float32Data[index] * depthInfo.rawValueToMeters;
```

GPU モードな場合は WebGLBindings から取得できます（explaner から引用）。

```js: DepthIndoの取得（GPUモード）
const view = ...;  // Obtained from a viewer pose.
const xrWebGLBinding = ...; // XRWebGLBinding created for the current session and GL context

const depthInfo = xrWebGLBinding.getDepthInformation(view);
```

深度画像は`depthInfo.texture`で取得出来るのでシェーダなどで処理をしますが、
各ピクセルは`luminance-alpha`だと uint16、`float32`だと float 型で扱う点だけ注意してください。

## 動作要件など

https://immersiveweb.dev/#supporttable

# Babylon.js での扱い方（2022 年 12 月時点）

## `scene.createDefaultXRExperienceAsync`では使えない

## `enterXRAsync`を呼び出して解決

# おわりに

## 参考文献

https://www.w3.org/TR/2022/WD-webxr-depth-sensing-1-20220419/

https://github.com/immersive-web/depth-sensing/

https://zenn.dev/drumath2237/scraps/9df1e32f7de989
